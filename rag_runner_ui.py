# rag_runner_ui.py â€” í†µí•©ë³¸ (Aì•ˆ ë‹¨ì¼ ë³´ê³ ì„œ, ì°¸ì¡° í™˜ìœ¨ë¯¸ì ìš©, ì›¹ìˆ˜ì§‘ ê°•í™”, ì›í™”(ë§Œì›) í‘œê¸°)

import os
import re
import json
import time
import hashlib
import warnings
import datetime
from pathlib import Path
from dataclasses import dataclass
from typing import List, Optional, Tuple, Dict, Any

import streamlit as st
from dotenv import load_dotenv

# ===== LangChain / Chroma =====
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain.tools.retriever import create_retriever_tool
from langchain_core.messages import AIMessage, HumanMessage, BaseMessage, SystemMessage
from langchain.schema import Document

# ===== HTML ë³¸ë¬¸ ì¶”ì¶œ =====
from bs4 import BeautifulSoup
try:
    import trafilatura  # optional
    HAS_TRAFILATURA = True
except Exception:
    trafilatura = None
    HAS_TRAFILATURA = False

# ===== DDG / SerpAPI (ì˜µì…˜) =====
try:
    from ddgs import DDGS  # new pkg
except Exception:
    try:
        from duckduckgo_search import DDGS  # legacy
    except Exception:
        DDGS = None

try:
    from serpapi import GoogleSearch  # optional
    HAS_SERPAPI = True
except Exception:
    HAS_SERPAPI = False

# ===== (ì„ íƒ) Chroma ìºì‹œ í´ë¦¬ì–´ =====
try:
    import chromadb
    chromadb.api.client.SharedSystemClient.clear_system_cache()
except Exception:
    pass

warnings.filterwarnings("ignore", category=DeprecationWarning)

# ===== í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ =====
load_dotenv()

# ===== LangSmith (ìˆìœ¼ë©´ ì‚¬ìš©) =====
langsmith_key = os.getenv("LANGCHAIN_API_KEY")
if langsmith_key:
    os.environ["LANGCHAIN_TRACING_V2"] = os.getenv("LANGCHAIN_TRACING_V2", "true")
    os.environ["LANGCHAIN_ENDPOINT"] = os.getenv("LANGCHAIN_ENDPOINT", "https://api.smith.langchain.com")
else:
    os.environ["LANGCHAIN_TRACING_V2"] = "false"

# ===== í†µí™” ë³€í™˜ ìœ í‹¸ (ë³¸ë¬¸ë§Œ ì ìš©) =====
def _krw_round_manwon(v_krw: float) -> str:
    """KRW ê°’ì„ ë§Œì› ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼í•´ '1,234ë§Œì›' í˜•íƒœ ë¬¸ìì—´ë¡œ ë°˜í™˜"""
    try:
        man = round(v_krw / 10_000)
        return f"{man:,}ë§Œì›"
    except Exception:
        return "ê¸ˆì•¡ ì˜¤ë¥˜"

def _usd_to_krw_amount(usd: float, rate: float) -> float:
    try:
        return usd * rate
    except Exception:
        return 0.0

def _extract_usd_amounts(text: str) -> List[Tuple[str, float]]:
    """
    ë³¸ë¬¸ì—ì„œ $1.5M, 500K, USD 2M ë“± USD ê¸ˆì•¡ íŒ¨í„´ì„ ì¶”ì¶œ.
    ë°˜í™˜: [(ì›ë¬¸í‘œí˜„, ê°’(USD float)), ...]
    """
    results = []
    # $1.5M / 1.5M / USD 1.5M / 500K / $500K / USD 500K / $2000000
    pattern = re.compile(
        r'(?:(?:USD|usd)\s*)?\$?\s*([0-9]+(?:\.[0-9]+)?)\s*([MmKk]?)',
        flags=re.IGNORECASE
    )
    # ë„ˆë¬´ ê´‘ë²”ìœ„í•œ ë§¤ì¹­ ë°©ì§€: ë‹¨ë… ìˆ«ìì§€ë§Œ ë‹¨ìœ„ ì—†ëŠ” ê²½ìš°ëŠ” ì œì™¸(ë¬¸ì¥ë¶€í˜¸/ë¬¸ë§¥ìœ¼ë¡œ ë³´ì •)
    for m in pattern.finditer(text):
        full = m.group(0)
        num = float(m.group(1))
        suf = (m.group(2) or "").upper()
        if suf == "M":
            usd = num * 1_000_000
        elif suf == "K":
            usd = num * 1_000
        else:
            # ë‹¨ìœ„ ì—†ëŠ” ìˆœìˆ˜ ìˆ«ìëŠ” '$' ë˜ëŠ” 'USD'ê°€ ë¶™ì€ ê²½ìš°ë§Œ ì¸ì •
            if not re.search(r'(USD|usd|\$)', full):
                continue
            usd = num
        # ë„ˆë¬´ ì‘ì€ ê°’(ì˜ˆ: $10)ì€ ë¬´ì‹œ -> ì˜¤ê²€ ë°©ì§€
        if usd < 1000:
            continue
        results.append((full.strip(), usd))
    return results

def _replace_usd_to_krw(body_md: str, rate: Optional[float] = None) -> str:
    """
    ë³¸ë¬¸(BODY)ì— í•œí•´ USDâ†’KRW(ë§Œì›) ë³‘ê¸°ë¥¼ ìˆ˜í–‰.
    ì°¸ê³ ìë£Œ(ì›ë¬¸) ë¯¸ë¦¬ë³´ê¸°ì—ëŠ” ì ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. (ë³¸ë¬¸ë§Œ í˜¸ì¶œ)
    """
    if rate is None:
        # ë³´ìˆ˜ì  ê³ ì • í™˜ìœ¨(ì›/ë‹¬ëŸ¬) â€” í•„ìš”ì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ì¡°ì •
        try:
            rate = float(os.getenv("USD_KRW_RATE", "1400"))
        except Exception:
            rate = 1400.0

    # ê°™ì€ ìœ„ì¹˜ì—ì„œ ì¤‘ë³µ ë³€í™˜ë˜ì§€ ì•Šë„ë¡, í° ë©ì–´ë¦¬ë¶€í„° ì¹˜í™˜
    found = _extract_usd_amounts(body_md)
    if not found:
        return body_md

    # ê¸´ ë¬¸ìì—´ì—ì„œ ì—¬ëŸ¬ ë²ˆ re.subí•˜ë©´ ì˜¤í”„ì…‹ì´ í”ë“¤ë¦´ ìˆ˜ ìˆìœ¼ë‹ˆ ìˆœì°¨ ì¹˜í™˜
    out = body_md
    used_spans: set = set()
    # ë’¤ì—ì„œ ì•ìœ¼ë¡œ ì¹˜í™˜í•˜ë©´ ì¸ë±ìŠ¤ ë³´ì¡´ì— ìœ ë¦¬í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¹˜í™˜ ì‚¬ìš©
    for raw, usd_val in sorted(found, key=lambda x: -len(x[0])):
        krw_val = _usd_to_krw_amount(usd_val, rate)
        krw_text = _krw_round_manwon(krw_val)
        # ì´ë¯¸ ë³‘ê¸°ëœ ê²½ìš° ë°©ì§€
        if f"{raw} (â‰ˆ" in out or f"{raw} (~" in out:
            continue
        out = out.replace(raw, f"{raw} (â‰ˆ{krw_text})")
    return out

# ===== ìœ í‹¸ =====
def _normalize_endpoint(url: Optional[str]) -> Optional[str]:
    return (url or None).rstrip("/") if url else url

def _get_env_any(*names: str, default: Optional[str] = None) -> Optional[str]:
    for n in names:
        v = os.getenv(n)
        if v:
            return v
    return default

# ===== Azure OpenAI ë¹Œë” =====
def build_embeddings() -> AzureOpenAIEmbeddings:
    endpoint = _normalize_endpoint(os.getenv("AZURE_OPENAI_ENDPOINT"))
    api_key = _get_env_any("AZURE_OPENAI_API_KEY", "AZURE_OPENAI_KEY")
    deployment = _get_env_any(
        "AZURE_OPENAI_EMBEDDING_DEPLOYMENT",
        "AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT",
        "AZURE_OPENAI_EMBEDDING",
        "AZURE_EMBEDDING_DEPLOYMENT",
        "AZURE_EMBED_DEPLOYMENT",
        "AZURE_EMBEDDING_MODEL",
        "AZURE_EMBEDDINGS_DEPLOYMENT",
        "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME"
    )
    api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-06-01")

    missing = []
    if not endpoint:   missing.append("AZURE_OPENAI_ENDPOINT")
    if not api_key:    missing.append("AZURE_OPENAI_API_KEY or AZURE_OPENAI_KEY")
    if not deployment: missing.append("AZURE_*_EMBEDDING_* (deployment/model) ì´ë¦„")
    if missing:
        st.error(f"âŒ Azure ì„ë² ë”© í™˜ê²½ë³€ìˆ˜ ë¶€ì¡±: {', '.join(missing)}")
        st.stop()

    return AzureOpenAIEmbeddings(
        azure_deployment=deployment,
        api_key=api_key,
        azure_endpoint=endpoint,
        api_version=api_version,
    )

def build_chat_llm() -> AzureChatOpenAI:
    endpoint = _normalize_endpoint(os.getenv("AZURE_OPENAI_ENDPOINT"))
    api_key = _get_env_any("AZURE_OPENAI_API_KEY", "AZURE_OPENAI_KEY")
    deployment = _get_env_any(
        "AZURE_OPENAI_CHAT_DEPLOYMENT",
        "AZURE_LLM_DEPLOYMENT",
        "AZURE_CHAT_DEPLOYMENT",
        "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME",
        "AZURE_OPENAI_MODEL_DEPLOYMENT"
    )
    api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-06-01")

    missing = []
    if not endpoint:   missing.append("AZURE_OPENAI_ENDPOINT")
    if not api_key:    missing.append("AZURE_OPENAI_API_KEY or AZURE_OPENAI_KEY")
    if not deployment: missing.append("AZURE_OPENAI_CHAT_DEPLOYMENT or AZURE_LLM_DEPLOYMENT or AZURE_CHAT_DEPLOYMENT")
    if missing:
        st.error(f"âŒ Azure ì±„íŒ… í™˜ê²½ë³€ìˆ˜ ë¶€ì¡±: {', '.join(missing)}")
        st.stop()

    return AzureChatOpenAI(
        model=deployment,
        api_key=api_key,
        azure_endpoint=endpoint,
        api_version=api_version,
        temperature=0,
    )

# ===== ê°„ë‹¨ ì—ì´ì „íŠ¸ =====
class SimpleGraph:
    def __init__(self, llm: AzureChatOpenAI):
        self.llm = llm
    def invoke(self, state: dict) -> dict:
        msgs = state.get("messages", [])
        reply = self.llm.invoke(msgs).content
        return {"messages": msgs + [AIMessage(content=reply)]}

def create_agent(retriever_tool, llm: AzureChatOpenAI):
    return SimpleGraph(llm)

# ===== ì„¸ì…˜ =====
def initialize_session_state():
    keys = [
        ("messages", [AIMessage(content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")]),
        ("vectorstore", None),
        ("current_files_hash", None),
        ("docs_info", None),
        ("graph", None),
        ("llm", None),
        ("loaded_once", False),
        ("system_prompt", None),
        ("final_report_md", None),
        ("report_topic", ""),
        ("decision_path", []),
        ("dr_policy", {}),
    ]
    for k, v in keys:
        if k not in st.session_state:
            st.session_state[k] = v

# ===== í´ë” ì²˜ë¦¬ =====
ALLOWED_EXTS = {".txt", ".pdf", ".md"}

def list_folder_files(folder_path: str) -> List[Path]:
    base = Path(folder_path).expanduser().resolve()
    if not base.exists() or not base.is_dir():
        return []
    files: List[Path] = []
    for p in base.rglob("*"):
        if p.is_file() and p.suffix.lower() in ALLOWED_EXTS:
            files.append(p)
    return sorted(files)

def get_folder_hash(files: List[Path]) -> str:
    sha = hashlib.sha256()
    for p in files:
        try:
            sha.update(str(p.resolve()).encode("utf-8"))
            sha.update(str(p.stat()).encode("utf-8"))
            with open(p, "rb") as f:
                sha.update(f.read())
        except Exception:
            continue
    return sha.hexdigest()

def _assert_embedding_ready(emb) -> None:
    try:
        vec = emb.embed_query("healthcheck")
        if not isinstance(vec, list) or len(vec) == 0:
            raise RuntimeError("ì„ë² ë”© ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.error(
            "âŒ ì„ë² ë”© í˜¸ì¶œ ì‹¤íŒ¨: Azure ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.\n"
            f"- AZURE_OPENAI_ENDPOINT={os.getenv('AZURE_OPENAI_ENDPOINT')}\n"
            f"- AZURE_OPENAI_API_VERSION={os.getenv('AZURE_OPENAI_API_VERSION')}\n"
            f"- ì„ë² ë”© ë°°í¬/ëª¨ë¸ ì´ë¦„(ì—¬ëŸ¬ í‚¤ í—ˆìš©)\n"
            f"ì›ì¸: {e}"
        )
        st.stop()

# ===== Vectorstore / Retriever =====
def process_folder(folder_path: str):
    files = list_folder_files(folder_path)
    if not files:
        st.info(f"í´ë”ì— ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
        return None, None

    current_hash = get_folder_hash(files)

    if st.session_state.current_files_hash == current_hash and st.session_state.vectorstore is not None:
        retriever = st.session_state.vectorstore.as_retriever()
        try:
            _ = retriever.invoke("health check")
        except Exception:
            pass
        docs_info = st.session_state.docs_info or [
            {"name": p.name, "type": ("application/pdf" if p.suffix.lower()==".pdf" else "text/plain"),
             "size": p.stat().st_size, "path": str(p)} for p in files
        ]
        desc = f"Search through: {', '.join(p['name'] for p in docs_info)}"
        retriever_tool = create_retriever_tool(retriever, "search_docs", desc)
        return retriever_tool, docs_info

    # ìƒˆë¡œìš´ ë¹Œë“œ
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=800, chunk_overlap=100)
    all_docs: List[Document] = []
    docs_info: List[dict] = []

    for p in files:
        try:
            info = {"name": p.name, "type": ("application/pdf" if p.suffix.lower()==".pdf" else "text/plain"),
                    "size": p.stat().st_size, "path": str(p)}
            if p.suffix.lower() in (".txt", ".md"):
                text = p.read_text(encoding="utf-8", errors="ignore")
                if text.strip():
                    raw = [Document(page_content=text, metadata={"source": p.name, "path": str(p)})]
                    all_docs.extend(splitter.split_documents(raw))
                    docs_info.append(info)
            elif p.suffix.lower() == ".pdf":
                loader = PyPDFLoader(str(p))
                raw_docs = loader.load()
                raw_docs = [d for d in raw_docs if d.page_content and d.page_content.strip()]
                for d in raw_docs:
                    d.metadata["source"] = p.name
                    d.metadata["path"] = str(p)
                if raw_docs:
                    all_docs.extend(splitter.split_documents(raw_docs))
                    docs_info.append(info)
        except Exception as e:
            st.error(f"Error processing {p.name}: {e}")

    if not all_docs:
        st.error("âŒ ì¸ë±ì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. KB í´ë”ì™€ í™•ì¥ì(.md/.txt/.pdf)ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return None, None

    embeddings = build_embeddings()
    _assert_embedding_ready(embeddings)

    persist_dir = os.getenv("VECTORSTORE_PATH", "").strip()
    collection_name = f"kb_{current_hash[:12]}"

    if persist_dir:
        vectorstore = Chroma.from_documents(
            documents=all_docs,
            embedding=embeddings,
            collection_name=collection_name,
            persist_directory=persist_dir,
        )
    else:
        vectorstore = Chroma.from_documents(
            documents=all_docs,
            embedding=embeddings,
            collection_name=collection_name,
        )

    st.session_state.vectorstore = vectorstore
    st.session_state.current_files_hash = current_hash
    st.session_state.docs_info = docs_info

    retriever = vectorstore.as_retriever()
    desc = f"Search through: {', '.join(d['name'] for d in docs_info)}"
    retriever_tool = create_retriever_tool(retriever, "search_docs", desc)
    return retriever_tool, docs_info

# ===== System í”„ë¡¬í”„íŠ¸(ê°•í™”) =====
def build_system_prompt(docs_info: List[dict]) -> str:
    file_names = ", ".join(d.get("name", "unknown") for d in (docs_info or []))
    # Few-shot í¬í•¨, ì „ë¬¸ì„±Â·ì •í™•ì„± ê°•ì¡°
    examples = """
ì˜ˆì‹œ 1:
ì§ˆë¬¸: "H100 8GPU ì„œë²„ì˜ ë©”ëª¨ë¦¬ ëŒ€ì—­í­ì€?"
ë‹µ: "ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ìˆ˜ì¹˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. HBM ëŒ€ì—­í­ ìˆ˜ì¹˜ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."

ì˜ˆì‹œ 2:
ì§ˆë¬¸: "A100ê³¼ H100ì˜ AI ì„±ëŠ¥ ì°¨ì´ëŠ”?"
ë‹µ: "ë¬¸ì„œì— êµ¬ì²´ ë²¤ì¹˜ë§ˆí¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€ ì„±ëŠ¥ ë¹„êµ ìë£Œ í•„ìš”."
"""
    return (
        "ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ HPC/GPU ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì „ë¬¸ ì˜ì—­ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤:\n"
        "- ì—”í„°í”„ë¼ì´ì¦ˆ GPU í´ëŸ¬ìŠ¤í„° ì„¤ê³„ (NVIDIA HGX, AMD MI ì‹œë¦¬ì¦ˆ)\n"
        "- InfiniBand/Ethernet ë„¤íŠ¸ì›Œí¬ í† í´ë¡œì§€ ìµœì í™”\n"
        "- ì „ë ¥/ëƒ‰ê° ì¸í”„ë¼ ì„¤ê³„\n"
        "- TCO ëª¨ë¸ë§ ë° ë²¤ë” í˜‘ìƒ ì „ëµ\n\n"
        "ë°˜ë“œì‹œ ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œë§Œ ê·¼ê±°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.\n"
        f"ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ: {file_names}\n\n"
        f"{examples}\n"
    )

# ===== 4í•„ë“œ í…œí”Œë¦¿ =====
CATEGORIES = [
    "ëŒ€ê·œëª¨ AI íŠ¸ë ˆì´ë‹ íŠ¹í™” (Hyperscale AI Training)",
    "ì¤‘í˜• ì—°êµ¬ HPC (Research HPC Cluster)",
    "ì¤‘í˜• ê¸°ê´€Â·ì‚°ì—… HPC (Enterprise/Institutional HPC)",
    "AI/LLM íŠ¹í™” ì„œë²„ (Fine-tuning & Inference)",
    "ì†Œê·œëª¨ ì—°êµ¬ì‹¤ ì„œë²„ (Lab-scale GPU Server)",
    "ë‹¨í’ˆ GPU/ì†Œí˜• ì„œë²„ (Single-GPU/Small-scale)",
]

DEFAULT_SECTIONS = [
    "ğŸ¯ ì¶”ì²œ êµ¬ì„± ìŠ¤í™",
    "ğŸ“Š ì„±ëŠ¥ ë° ìš©ëŸ‰ ë¶„ì„",
    "ğŸ’° ë¹„ìš© ë¶„ì„ ë° TCO",
    "âš–ï¸ ëŒ€ì•ˆ ë° ë¹„êµ ë¶„ì„",
    "ğŸ› ï¸ êµ¬í˜„ ë¡œë“œë§µ",
    "âœ… ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­",
    "ğŸ“š ì°¸ê³ ìë£Œ(ì›ë¬¸)",
]

def build_quick_query_prompt(category: str, gpu: str, users: str, cost: str) -> str:
    gpu_hints = []
    g = (gpu or "").lower()
    if "h100" in g:
        gpu_hints.extend(["H100 80GB", "HGX H100", "8x H100", "8Ã— H100", "NVLink", "SXM"])
    if ("8" in g and "100" in g) or "8x" in g or "8Ã—" in g:
        gpu_hints.append("8-GPU")
    parts = [
        f"ì„±í–¥/ìš©ë„: {category}",
        f"GPU ìŠ¤íƒ: {gpu}",
        f"ì˜ˆìƒ ì‚¬ìš©ì ìˆ˜: {users}",
        f"ì˜ˆì‚°/ë¹„ìš© ë²”ìœ„: {cost}",
        "ê²€ìƒ‰ íŒíŠ¸: (HGX H100 OR H100 80GB OR 8x H100 OR 8Ã— H100 OR NVLink) "
        "AND (cost OR budget OR price OR TCO) AND (user OR seat OR concurrency) "
        "AND (training OR LLM OR fine-tune OR pretraining OR inference)",
        f"í‚¤ì›Œë“œ íŒíŠ¸: {', '.join(gpu_hints)}" if gpu_hints else "",
    ]
    return "\n".join([p for p in parts if p])

def build_topic_title(category: str, gpu: str, users: str, cost: str) -> str:
    c = (category or "").strip() or "ìš”êµ¬ì‚¬í•­"
    g = (gpu or "").strip() or "GPU N/A"
    u = (users or "").strip() or "ì‚¬ìš©ì N/A"
    b = (cost or "").strip() or "ì˜ˆì‚° N/A"
    return f"{c} | {g} | ì‚¬ìš©ì {u} | ì˜ˆì‚° {b}"

# ===== ìì—°ì–´ ìš”êµ¬ì‚¬í•­ íŒŒì„œ =====
def parse_freeform_requirements(text: str) -> Dict[str, str]:
    t = (text or "").strip()
    out = {"category": "", "gpu": "", "users": "", "cost": "", "special": ""}
    if not t:
        return out

    m = re.search(r"(HGX\s*H100|H100|A100|H200|L40S|RTX\s*6000|MI300)[^\n,;]*", t, re.IGNORECASE)
    out["gpu"] = m.group(0) if m else ""

    m = re.search(r"(\d+\s*[â€“\-~]\s*\d+|\d+)\s*(?:ëª…|users?)", t, re.IGNORECASE)
    out["users"] = (m.group(1) if m else "").replace(" ", "")

    m = re.search(r"(\$\s?\d+(\.\d+)?\s*[MK]|[0-9]+(?:\s?ì–µ)?\s?ì›|\d+\s?K|\d+\s?M|USD\s?\d+(\.\d+)?\s*[MK])", t, re.IGNORECASE)
    out["cost"] = m.group(1) if m else ""

    low = t.lower()
    if any(k in low for k in ["hyperscale", "ëŒ€ê·œëª¨", "pretrain"]):
        out["category"] = CATEGORIES[0]
    elif any(k in low for k in ["ì—°êµ¬", "research", "hpc"]):
        out["category"] = CATEGORIES[1]
    elif any(k in low for k in ["ê¸°ê´€", "enterprise", "institution"]):
        out["category"] = CATEGORIES[2]
    elif any(k in low for k in ["inference", "fine-tune", "ì¶”ë¡ ", "íŒŒì¸íŠœë‹"]):
        out["category"] = CATEGORIES[3]
    elif any(k in low for k in ["ì—°êµ¬ì‹¤", "lab", "ì†Œê·œëª¨"]):
        out["category"] = CATEGORIES[4]
    else:
        out["category"] = CATEGORIES[0]

    out["special"] = t
    return out

# ===== Evidence êµ¬ì¡° =====
@dataclass
class EvidenceItem:
    text: str
    source: str
    title: Optional[str] = None
    fetched_at: Optional[str] = None
    score: Optional[float] = None  # RAG ì ìˆ˜(ê°„ì´)

# ===== ë³´ê³ ì„œ ì»´í¬ì € =====
SECTION_EXPERT_PROMPTS = {
    "ğŸ¯ ì¶”ì²œ êµ¬ì„± ìŠ¤í™": """
ë‹¹ì‹ ì€ NVIDIA Elite íŒŒíŠ¸ë„ˆì‚¬ì˜ ìˆ˜ì„ ì†”ë£¨ì…˜ ì•„í‚¤í…íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê´€ì ì—ì„œ í‘œë¥¼ ì‘ì„±í•˜ì„¸ìš”:

ê¸°ìˆ ì  ì •í™•ì„±:
- ì •í™•í•œ íŒŒíŠ¸ ë„˜ë²„ (ì˜ˆ: NVIDIA HGX H100 8-GPU, Dell PowerEdge XE9680)
- í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤ ê²€ì¦
- ì¸ì¦ëœ êµ¬ì„±ë§Œ ê¶Œì¥

ë¹„ì¦ˆë‹ˆìŠ¤ ê³ ë ¤ì‚¬í•­:
- ê³µê¸‰ ê°€ëŠ¥ì„± ë° ë¦¬ë“œíƒ€ì„
- í™•ì¥ì„± ë¡œë“œë§µ
- ë²¤ë” ì§€ì› ë ˆë²¨

í‘œ í˜•ì‹:
| êµ¬ì„±ìš”ì†Œ | ê¶Œì¥ ì‚¬ì–‘(ëª¨ë¸/ìˆ˜ëŸ‰/í† í´ë¡œì§€) | ê·¼ê±° [n] | ë¹„ê³  |
""",
    "ğŸ’° ë¹„ìš© ë¶„ì„ ë° TCO": """
ë‹¹ì‹ ì€ Fortune 500 IT ì¡°ë‹¬ ë‹´ë‹¹ìì…ë‹ˆë‹¤.
ë‹¤ìŒ TCO ëª¨ë¸ì„ ì ìš©í•˜ì„¸ìš”:

ì´ˆê¸° íˆ¬ì (CAPEX):
- í•˜ë“œì›¨ì–´ ì •ê°€ vs í˜‘ìƒ ì˜ˆìƒê°€
- ì„¤ì¹˜/êµ¬ì„± ë¹„ìš©
- ì´ˆê¸° êµìœ¡/ì¸ì¦ ë¹„ìš©

ìš´ì˜ë¹„ (OPEX):
- ì „ë ¥ë¹„ (kWh ë‹¨ê°€ í¬í•¨)
- ëƒ‰ê°ë¹„ (PUE ê³ ë ¤)
- ìœ ì§€ë³´ìˆ˜ (ì—°ê°„ %ë¡œ ì‚°ì •)
- ì¸ê±´ë¹„ (FTE ê¸°ì¤€)

3-5ë…„ ì´ ë¹„ìš©ê³¼ ëŒ€ì•ˆ ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµ í•„ìˆ˜.
"""
}

def _plan_outline(llm: AzureChatOpenAI, topic: str) -> Dict:
    pythonsys = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ HPC/GPU ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì „ë¬¸ ì˜ì—­ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤:
- ì—”í„°í”„ë¼ì´ì¦ˆ GPU í´ëŸ¬ìŠ¤í„° ì„¤ê³„ (NVIDIA HGX, AMD MI ì‹œë¦¬ì¦ˆ)
- InfiniBand/Ethernet ë„¤íŠ¸ì›Œí¬ í† í´ë¡œì§€ ìµœì í™”
- ì „ë ¥/ëƒ‰ê° ì¸í”„ë¼ ì„¤ê³„
- TCO ëª¨ë¸ë§ ë° ë²¤ë” í˜‘ìƒ ì „ëµ

ëª©ì°¨ ìƒì„± ì‹œ ë‹¤ìŒì„ ì¤€ìˆ˜í•˜ì„¸ìš”:
1. ê¸°ìˆ ì  ì •í™•ì„±: ì‹¤ì œ ì œí’ˆ ëª¨ë¸ëª…, ì •í™•í•œ ì‚¬ì–‘, í˜¸í™˜ì„± ê³ ë ¤
2. ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì : ROI, í™•ì¥ì„±, ë¦¬ìŠ¤í¬ ë¶„ì„ í¬í•¨
3. êµ¬í˜„ ê°€ëŠ¥ì„±: í˜„ì‹¤ì  ì¼ì •, ê³µê¸‰ë§, ì¸ë ¥ ìš”êµ¬ì‚¬í•­
4. ì‚°ì—… í‘œì¤€: IEEE, OCP, Green500 ë“± í‘œì¤€ ì¤€ìˆ˜

JSON í˜•ì‹: {"title": str, "sections": [{"heading": str, "focus": str, "technical_depth": str}]}"""
    sys = pythonsys
    user = (f"ì£¼ì œ: {topic}\nìš”êµ¬ì‚¬í•­: {', '.join(DEFAULT_SECTIONS)} í¬í•¨")
    out = llm.invoke([SystemMessage(content=sys), HumanMessage(content=user)]).content
    txt = out.strip().strip("```").replace("json", "")
    try:
        data = json.loads(txt)
        heads = []
        for s in data.get("sections", []):
            if isinstance(s, dict):
                h = s.get("heading") or s.get("title", "")
            else:
                h = str(s)
            if h:
                heads.append(h)
        if not heads:
            heads = DEFAULT_SECTIONS
        return {"title": data.get("title") or f"{topic} â€” ì‹œìŠ¤í…œ êµ¬ì„± ì „ë¬¸ ë³´ê³ ì„œ", "sections": heads}
    except Exception:
        return {"title": f"{topic} â€” ì‹œìŠ¤í…œ êµ¬ì„± ì „ë¬¸ ë³´ê³ ì„œ", "sections": DEFAULT_SECTIONS}

def _short_preview(text: str, maxlen: int = 200) -> str:
    if not text:
        return ""
    t = re.sub(r'\s+', ' ', text)
    # ì›ë¬¸ ìœ ì§€(í™˜ìœ¨ ë³‘ê¸° ê¸ˆì§€), íŠ¹ìˆ˜ë¬¸ìëŠ” ê°€ë³ê²Œ ì •ë¦¬ë§Œ
    t = re.sub(r'[\r\t]', ' ', t)
    return (t[:maxlen] + "...") if len(t) > maxlen else t

def _compose_enhanced_markdown_from_evidence(
    llm: AzureChatOpenAI,
    topic: str,
    evidence: List[EvidenceItem],
    outline: Optional[Dict] = None,
    min_sent: int = 8,
    max_sent: int = 15,
) -> str:
    """
    RAG/DeepResearch í†µí•© Evidenceë¡œ í•œ ë²ˆë§Œ ì‘ì„±.
    - ë³¸ë¬¸(ì„¹ì…˜)ì€ ì›í™” ë³‘ê¸° ë³€í™˜ ì ìš©
    - ì°¸ê³ ìë£Œ(ì›ë¬¸)ëŠ” ë³€í™˜ ê¸ˆì§€ (ì›ë¬¸ ê·¸ëŒ€ë¡œ)
    """
    # Evidence íŒ¨í‚¹(LLM ì¸ìš©ë²ˆí˜¸ ê¸°ì¤€)
    packed = []
    for i, e in enumerate(evidence, 1):
        ttl = (e.title or e.source or f"evidence-{i}").replace("KB|", "").replace("WEB|", "")
        snippet = (e.text or "").strip().replace("\n\n", "\n")
        src = (e.source or "").replace("KB|", "").replace("WEB|", "")
        packed.append(f"[{i}] {ttl}\nSRC: {src}\n{snippet}")
    ev_text = "\n\n".join(packed) if packed else "No evidence."

    if outline is None:
        outline = _plan_outline(llm, topic)
    wanted = set(DEFAULT_SECTIONS)
    sections = []
    seen_h = set()
    for h in outline.get("sections", []):
        if h in wanted and h not in seen_h:
            sections.append(h)
            seen_h.add(h)

    # ë³¸ë¬¸(body) êµ¬ì„±
    body_parts = []
    body_parts.append(f"# {outline['title']}\n")
    body_parts.append(f"> **ìƒì„± ì‹œê°(UTC):** {datetime.datetime.now(datetime.timezone.utc).isoformat().replace('+00:00','Z')}\n")
    body_parts.append(f"> **ë¶„ì„ ëŒ€ìƒ:** {topic}\n")
    body_parts.append(f"> **Evidence ì†ŒìŠ¤ ìˆ˜:** {len(evidence)}ê°œ\n\n")

    sys = (
        "ë„ˆëŠ” 'HPC/GPU ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì „ë¬¸ê°€'ë‹¤. ì•„ë˜ Evidenceë§Œì„ ê·¼ê±°ë¡œ í•œêµ­ì–´ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ë¼.\n"
        "ìš”êµ¬ì‚¬í•­: 1) êµ¬ì²´ ëª¨ë¸/ìˆ˜ëŸ‰/í† í´ë¡œì§€ 2) ê° ì£¼ì¥ ë’¤ [n] ì¸ìš©ë²ˆí˜¸ 3) Evidence ë°– ì§€ì‹ ê¸ˆì§€ "
        "4) ë¶ˆí™•ì‹¤ì€ 'ì¶”ê°€ ê²€í†  í•„ìš”' ëª…ì‹œ 5) ì„¹ì…˜ëª… ì¤‘ë³µ ê¸ˆì§€ 6) í‘œ ì„¹ì…˜ì€ í‘œë§Œ ì‘ì„±"
    )
    section_prompts = SECTION_EXPERT_PROMPTS

    for heading in sections:
        section_specific = section_prompts.get(heading, "")
        prompt = (
            f"ì£¼ì œ: {topic}\n"
            f"ì„¹ì…˜: {heading}\n\n"
            f"{section_specific}\n\n"
            f"Evidence:\n{ev_text}\n\n"
            f"ìš”êµ¬ì‚¬í•­: {min_sent}~{max_sent}ë¬¸ì¥(í‘œ ì„¹ì…˜ì€ í‘œë§Œ), êµ¬ì²´ ìˆ˜ì¹˜/ëª¨ë¸, ëª¨ë“  ì£¼ì¥ ëì— [n] ì¸ìš©ë²ˆí˜¸, Evidence ë°– ì§€ì‹ ê¸ˆì§€."
        )
        try:
            rsp = llm.invoke([SystemMessage(content=sys), HumanMessage(content=prompt)]).content
            body_parts.append(f"## {heading}\n\n{rsp.strip()}\n")
        except Exception as e:
            body_parts.append(f"## {heading}\n\nâš ï¸ ì„¹ì…˜ ìƒì„± ì˜¤ë¥˜: {e}\n")

    # ë³¸ë¬¸ë§Œ í™˜ìœ¨ ë³€í™˜
    body_md = "\n".join(body_parts)
    body_md = _replace_usd_to_krw(body_md)

    # ì°¸ê³ ìë£Œ(ì›ë¬¸) â€” ë³€í™˜ ê¸ˆì§€
    refs_parts = []
    if evidence:
        refs_parts.append("\n---\n")
        refs_parts.append("## ğŸ“š ì°¸ê³ ìë£Œ(ì›ë¬¸)\n")
        refs_parts.append("### ğŸ“„ ë¬¸ì„œ ëª©ë¡\n")
        seen = set()
        for i, e in enumerate(evidence, 1):
            title = (e.title or "ì œëª© ì—†ìŒ").replace("KB|", "").replace("WEB|", "")
            source = (e.source or "").replace("KB|", "").replace("WEB|", "")
            key = (title.strip().lower(), source.strip().lower())
            if key in seen:
                continue
            seen.add(key)
            fetched = f" (ìˆ˜ì§‘: {e.fetched_at})" if e.fetched_at else ""
            preview = _short_preview(e.text or "", maxlen=200)
            refs_parts.append(f"[{i}] {title}\n")
            refs_parts.append(f"- **ì¶œì²˜:** {source}{fetched}\n")
            if preview:
                refs_parts.append(f"- **ë¯¸ë¦¬ë³´ê¸°:** {preview}\n")
            refs_parts.append("")
    refs_md = "\n".join(refs_parts)

    return body_md + refs_md

# ===== RAG ê²€ìƒ‰ =====
def run_rag(query: str) -> List[Document]:
    if st.session_state.vectorstore is None:
        return []
    retriever = st.session_state.vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 15, "fetch_k": 60})
    try:
        docs = retriever.invoke(query)
    except Exception:
        docs = retriever.get_relevant_documents(query)
    return docs

def _evidence_from_docs(docs: List[Document], query: str, keep_len: int = 1200) -> List[EvidenceItem]:
    items: List[EvidenceItem] = []
    q_terms = [t for t in re.split(r"\W+", query.lower()) if t]
    for i, d in enumerate(docs):
        if not d.page_content:
            continue
        meta = d.metadata or {}
        content_lower = d.page_content.lower()
        score = sum(1 for term in q_terms if term in content_lower) / max(1, len(q_terms))
        items.append(EvidenceItem(
            text=d.page_content[:keep_len],
            source=f"KB|{meta.get('path') or meta.get('source') or meta.get('file_path') or meta.get('url') or f'doc_{i}'}",
            title=meta.get("title") or meta.get("page") or meta.get("source") or meta.get("path"),
            fetched_at=meta.get("fetched_at") or meta.get("date") or datetime.datetime.now(datetime.timezone.utc).isoformat(),
            score=score,
        ))
    return items

# ===== Deep Research (ì›¹ ìˆ˜ì§‘: ê°•í™” í•„í„° í¬í•¨) =====
def _domain_of(u: str) -> str:
    try:
        from urllib.parse import urlparse
        p = urlparse(u)
        return (p.netloc or "").lower()
    except Exception:
        return ""

HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

BLOCKED_DOMAINS = {
    "hyperscale.com", "dupontregistry.com", "funbe", "tkor", "namu.wiki",
    "redrosa.co.kr", "urimal.org", "bing.com", "aclick", "jwd1.org",
    "dataannotation.tech", "fliphtml5.com", "butterflyinvest.com",
    "makeit27.com", "blog.makeit27.com", "wipi.ai.kr", "mindpath.tistory.com",
    "uproger.com", "technical.city", "youtube.com", "www.youtube.com",
    "bing.com/aclick"
}

TRUSTED_DOMAINS = {
    "nvidia.com", "developer.nvidia.com", "docs.nvidia.com", "blogs.nvidia.com",
    "dell.com", "www.dell.com",
    "supermicro.com", "www.supermicro.com",
    "hpe.com", "www.hpe.com",
    "lenovo.com", "www.lenovo.com",
    "microsoft.com", "learn.microsoft.com", "azure.microsoft.com",
    "techcommunity.microsoft.com",
    "arxiv.org", "ieee.org",
    "idtechex.com",
    "ktcloud.com", "tech.ktcloud.com",
    "tomshardware.com"
}

ALLOWED_HINTS = ["nvidia", "dell", "h100", "hgx", "nvlink", "a100", "gpu", "h200", "xe9680", "tco", "price", "budget"]

REQUIRED_HINTS = {
    "gpu","nvidia","h100","a100","h200","hgx","nvlink","nvswitch",
    "infiniband","ndr","llm","training","pretrain","tco","cluster",
    "xe9680","mi300","cdna3","cuda","pytorch","tensor core","sxm","pcie"
}

def _safe_fetch(url: str, timeout=20) -> Tuple[str, Optional[str]]:
    try:
        import requests
        r = requests.get(url, headers=HEADERS, timeout=timeout, allow_redirects=True)
        r.raise_for_status()
        if HAS_TRAFILATURA:
            try:
                downloaded = trafilatura.fetch_url(r.url, no_ssl=True, timeout=timeout)
                if downloaded:
                    txt = trafilatura.extract(downloaded, include_comments=False, include_tables=False, include_images=False)
                    if txt and len(txt.split()) > 50:
                        soup = BeautifulSoup(r.text, "html.parser")
                        t = soup.find("meta", {"property": "article:published_time"}) or soup.find("time")
                        published = (t.get("content") if t and t.has_attr("content") else (t.get_text(strip=True) if t else None))
                        return txt.strip(), published
            except Exception:
                pass
        soup = BeautifulSoup(r.text, "html.parser")
        for s in soup(["script", "style", "noscript"]):
            s.extract()
        txt = soup.get_text("\n", strip=True)
        if len(txt.split()) < 50:
            return "", None
        t = soup.find("meta", {"property": "article:published_time"}) or soup.find("time")
        published = (t.get("content") if t and t.has_attr("content") else (t.get_text(strip=True) if t else None))
        return txt, published
    except Exception:
        return "", None

def _search_ddg(q: str, max_results=10, timeframe_days=365) -> List[Dict[str, Any]]:
    if DDGS is None:
        return []
    since = (datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=timeframe_days)).date().isoformat()
    hint_q = f"{q} after:{since}"
    out: List[Dict[str, Any]] = []
    try:
        with DDGS() as ddgs:
            for r in ddgs.text(hint_q, region="kr-kr", max_results=max_results):
                if not r or "href" not in r:
                    continue
                out.append({
                    "title": (r.get("title") or "").strip(),
                    "url": (r.get("href") or "").split("#")[0].strip(),
                    "snippet": r.get("body") or r.get("snippet") or "",
                    "source": r.get("source", "duckduckgo"),
                    "published": None,
                })
    except Exception:
        return []
    return out

def _search_serpapi(q: str, max_results=10, timeframe_days=365, gl="kr", hl="ko") -> List[Dict[str, Any]]:
    if not HAS_SERPAPI or not os.getenv("SERPAPI_API_KEY"):
        return []
    since = (datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(days=timeframe_days)).date().isoformat()
    try:
        params = {"engine": "google", "q": f"{q} after:{since}", "gl": gl, "hl": hl, "num": min(max_results, 20), "api_key": os.getenv("SERPAPI_API_KEY")}
        results = GoogleSearch(params).get_dict()
        out = []
        for it in results.get("organic_results", []):
            out.append({
                "title": it.get("title", ""),
                "url": (it.get("link") or "").split("#")[0].strip(),
                "snippet": it.get("snippet", ""),
                "source": "google",
                "published": it.get("date", ""),
            })
        return out
    except Exception:
        return []

@dataclass
class SourceDoc:
    url: str
    title: str
    snippet: str
    published: Optional[str]
    text: str
    domain: str

def _normalize_topic_for_web(topic: str) -> str:
    """ì›¹ ê²€ìƒ‰ìš©ìœ¼ë¡œ ë¼ë²¨/ë¶ˆìš©ì–´ ì œê±°."""
    lines = [l.strip() for l in topic.splitlines() if l.strip()]
    keep = []
    for l in lines:
        l = re.sub(r"^(ì„±í–¥/ìš©ë„|GPU\s*ìŠ¤íƒ|GPU\s*ìŠ¤í™|ì˜ˆìƒ\s*ì‚¬ìš©ì\s*ìˆ˜|ì˜ˆì‚°/ë¹„ìš©\s*ë²”ìœ„|íŠ¹ë³„\s*ìš”êµ¬ì‚¬í•­)\s*:\s*", "", l, flags=re.IGNORECASE)
        l = re.sub(r"^(ê²€ìƒ‰\s*íŒíŠ¸|í‚¤ì›Œë“œ\s*íŒíŠ¸)\s*:\s*", "", l, flags=re.IGNORECASE)
        l = l.replace("ì„±í–¥", "")
        keep.append(l)
    q = " ".join(keep)
    q = re.sub(r"\s+", " ", q).strip()
    return q

def _good_domain(domain: str) -> bool:
    d = domain.lower()
    if any(b in d for b in BLOCKED_DOMAINS):
        return False
    if d in TRUSTED_DOMAINS:
        return True
    if len(d) < 4 or "." not in d:
        return False
    return not any(x in d for x in ["aclick", "adservice", "doubleclick", "utm"])

def _has_required_hints(text: str, min_hits: int = 2) -> bool:
    t = text.lower()
    hits = sum(1 for k in REQUIRED_HINTS if k in t)
    return hits >= min_hits

def _gather_sources(topic: str, policy: Dict[str, Any]) -> List[SourceDoc]:
    base = _normalize_topic_for_web(topic)

    tokens = []
    gmatch = re.findall(r"(HGX\s*H100|H100|A100|H200|MI300|XE9680|NVLINK|NVSWITCH|INFINIBAND)", base, flags=re.IGNORECASE)
    if gmatch:
        tokens.extend(gmatch)
    anchor = ' '.join(tokens) if tokens else base

    queries = [
        base,
        f"{anchor} TCO budget price",
        f"{anchor} NVLink InfiniBand cluster",
        f"{anchor} site:nvidia.com",
        f"{anchor} site:dell.com",
        f"{anchor} filetype:pdf",
    ]

    timeframe = int(policy.get("timeframe_days", 365))
    max_each = int(policy.get("min_sources", 12)) * 2

    pool: List[Dict[str, Any]] = []
    for q in queries:
        pool.extend(_search_ddg(q, max_results=10, timeframe_days=timeframe))
        pool.extend(_search_serpapi(q, max_results=10, timeframe_days=timeframe))
        time.sleep(0.15)

    seen = set()
    cands: List[Dict[str, Any]] = []
    for it in pool:
        url = (it.get("url") or "").split("#")[0].strip()
        if not url:
            continue
        dom = _domain_of(url)
        if not _good_domain(dom):
            continue

        title = it.get("title", "") or ""
        snippet = it.get("snippet", "") or ""
        title_snip = (title + " " + snippet)

        # GPU/HPC í‚¤ì›Œë“œ ìµœì†Œ ë§¤ì¹­
        if not _has_required_hints(title_snip, min_hits=1):
            continue

        key = url
        if key in seen:
            continue
        seen.add(key)

        base_hint = sum(1 for h in REQUIRED_HINTS if h in title_snip.lower())
        trust_bonus = 3 if dom in TRUSTED_DOMAINS else 0
        it["_score"] = base_hint + trust_bonus

        cands.append(it)

    cands.sort(key=lambda x: x.get("_score", 0), reverse=True)
    cands = cands[:max_each]

    docs: List[SourceDoc] = []
    for it in cands:
        url = it["url"]
        text, published = _safe_fetch(url)
        if not text or len(text.split()) < 120:
            continue
        if not _has_required_hints(text, min_hits=2):
            continue
        dom = _domain_of(url)
        docs.append(
            SourceDoc(
                url=url,
                title=it.get("title") or url,
                snippet=it.get("snippet", ""),
                published=published,
                text=text,
                domain=dom
            )
        )
        if len(docs) >= max_each:
            break
    return docs

# === Deep Research â†’ Evidence ë³€í™˜ ===
def gather_dr_evidence(topic: str, policy: Dict[str, Any], keep_len: int = 1200) -> List[EvidenceItem]:
    web_docs = _gather_sources(topic, policy)
    ev: List[EvidenceItem] = []
    for d in web_docs:
        ev.append(EvidenceItem(
            text=(d.text or "")[:keep_len],
            source=f"WEB|{d.url}",
            title=d.title or d.url,
            fetched_at=d.published or datetime.datetime.now(datetime.timezone.utc).isoformat(),
            score=None
        ))
    return ev

# === Evidence ì¤‘ë³µ/ë„ë©”ì¸ ì œí•œ ===
def dedup_evidence(evidence: List[EvidenceItem]) -> List[EvidenceItem]:
    seen = set()
    out: List[EvidenceItem] = []
    for e in evidence:
        key = ((e.title or "").strip().lower(), (e.source or "").strip().lower())
        if key in seen:
            continue
        seen.add(key)
        out.append(e)
    return out

def _limit_refs_by_domain(evidence: List[EvidenceItem], max_total=12, max_per_domain=3) -> List[EvidenceItem]:
    by_domain: Dict[str, int] = {}
    out = []
    for e in evidence:
        src = (e.source or "")
        dom = _domain_of(src.replace("WEB|","")) if "WEB|" in src else "KB"
        cnt = by_domain.get(dom, 0)
        if cnt >= max_per_domain:
            continue
        by_domain[dom] = cnt + 1
        out.append(e)
        if len(out) >= max_total:
            break
    return out

# ===== ìš”êµ¬ì‚¬í•­ ë¶„ì„ & ì¶©ë¶„ì„± íŒì • =====
def analyze_requirements(category: str, gpu: str, users: str, cost: str, special_req: str) -> Dict[str, Any]:
    txt = " ".join([category or "", gpu or "", users or "", cost or "", special_req or ""]).lower()
    kws = [t for t in re.split(r"\W+", txt) if t]
    return {"keywords": kws[:64], "raw": txt}

def assess_sufficiency(docs: List[Document], req: Dict[str, Any]) -> Tuple[bool, Dict[str, Any]]:
    if not docs:
        return False, {"hits": 0, "covered_keywords": 0, "need": {"hits": 7, "kw": 6}}
    content = "\n".join([d.page_content[:500].lower() for d in docs if d.page_content])
    covered = sum(1 for k in set(req["keywords"]) if k and k in content)
    enough = (len(docs) >= 7) and (covered >= max(6, len(set(req["keywords"])) // 5))
    return enough, {"hits": len(docs), "covered_keywords": covered, "need": {"hits": 7, "kw": 6}}

# ===== Streamlit ì•± =====
st.set_page_config(page_title="TTD-DR HPC/GPU System Designer", layout="wide")
st.title("ğŸ§¬ GPU ì¸í”„ë¼ TAë¥¼ ìœ„í•œ ì§€ëŠ¥í˜• ë³´ê³ ì„œ Agent")

initialize_session_state()
if st.session_state.llm is None:
    try:
        st.session_state.llm = build_chat_llm()
    except Exception as e:
        st.error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ì‚¬ì´ë“œë°”
st.sidebar.header("ğŸ“ Knowledge Base")
default_folder = os.getenv("KNOWLEDGE_BASE_PATH") or os.getenv("PDF_DOCUMENTS_DIR") or "./documents"
folder_path = st.sidebar.text_input("ë¬¸ì„œ í´ë” ê²½ë¡œ", value=str(Path(default_folder).resolve()))
scan = st.sidebar.button("ğŸ”„ í´ë” ìŠ¤ìº”/ì¬ë¡œë”©")

st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ” ë”¥ë¦¬ì„œì¹˜(ë‚´ì¥)")
timeframe_days = st.sidebar.number_input("ìµœê·¼ Nì¼", min_value=30, max_value=3650, value=int(os.getenv("DR_TIMEFRAME_DAYS", "365")))
min_sources = st.sidebar.number_input("ìµœì†Œ ì¶œì²˜ ìˆ˜", min_value=4, max_value=100, value=int(os.getenv("DR_MIN_SOURCES", "12")))
st.session_state.dr_policy = dict(timeframe_days=timeframe_days, min_sources=min_sources)

# ì´ˆê¸° ë¡œë“œ/ìŠ¤ìº”
if (not st.session_state.loaded_once) or scan:
    with st.spinner("ğŸ“‚ í´ë” ìŠ¤ìº” ì¤‘..."):
        retriever_tool, docs_info = process_folder(folder_path)
        if retriever_tool and docs_info:
            try:
                if st.session_state.llm is None:
                    st.session_state.llm = build_chat_llm()
                st.session_state.graph = create_agent(retriever_tool, st.session_state.llm)
            except Exception as e:
                st.error(f"âŒ ì—ì´ì „íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            st.session_state.system_prompt = build_system_prompt(docs_info)
            st.success(f"âœ… {len(docs_info)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!")
            msg = "\n".join([
                f"ğŸ“š í´ë”ì—ì„œ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤: {folder_path}",
                *[f"- {d['name']} ({'PDF' if 'pdf' in d['type'].lower() else 'Text'}, {d['size']/1024:.1f}KB)" for d in docs_info],
                "\nì´ì œ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!",
            ])
            st.session_state.messages.append(AIMessage(content=msg))
            st.session_state.loaded_once = True

# ê¸°ì¡´ ëŒ€í™” í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message("assistant" if isinstance(message, AIMessage) else "user"):
        st.write(message.content)

# ===== ì…ë ¥ UI =====
st.markdown("### ğŸ¯ ì‹œìŠ¤í…œ êµ¬ì„± ìš”êµ¬ì‚¬í•­ ì…ë ¥")
with st.form(key="enhanced_query_form", clear_on_submit=False):
    col1, col2 = st.columns(2)
    with col1:
        category = st.selectbox("ì„±í–¥/ìš©ë„", CATEGORIES, index=0)
        gpu = st.text_input("GPU ìŠ¤í™", value="HGX H100 8Ã—80GB")
    with col2:
        users = st.text_input("ì˜ˆìƒ ì‚¬ìš©ì ìˆ˜", value="80â€“100ëª…")
        cost = st.text_input("ì´ ì˜ˆì‚°", value="$1.5â€“2M")
    special_req = st.text_area("íŠ¹ë³„ ìš”êµ¬ì‚¬í•­(ì„ íƒ)", placeholder="ì˜ˆ: ê³ ê°€ìš©ì„±, DR, íŠ¹ì • SW í˜¸í™˜ ë“±", height=80)
    nl_text = st.text_area("âœï¸ ìì—°ì–´ ìš”êµ¬ì‚¬í•­ (ì„ íƒ) â€” ì…ë ¥ ì‹œ ìœ„ 4í•„ë“œë¥¼ ìë™ ë³´ì™„/ë®ì–´ì“°ê¸°", height=100)
    submitted = st.form_submit_button("ğŸš€ TTD-DR ë³´ê³ ì„œ ìƒì„±", type="primary")

if submitted:
    st.session_state.decision_path = []

    if nl_text and nl_text.strip():
        parsed = parse_freeform_requirements(nl_text)
        category = parsed["category"] or category
        gpu      = parsed["gpu"] or gpu
        users    = parsed["users"] or users
        cost     = parsed["cost"] or cost
        if parsed["special"]:
            special_req = parsed["special"]

    req = analyze_requirements(category, gpu, users, cost, special_req)
    st.info("ìš”êµ¬ì‚¬í•­ í‚¤ì›Œë“œ: " + ", ".join(req["keywords"][:12]))

    search_query_text = build_quick_query_prompt(category, gpu, users, cost)
    if special_req:
        search_query_text += f"\níŠ¹ë³„ ìš”êµ¬ì‚¬í•­: {special_req}"

    topic_title = build_topic_title(category, gpu, users, cost)

    with st.spinner("ğŸ” RAG ê²€ìƒ‰ ì¤‘â€¦"):
        docs = run_rag(search_query_text)
    st.success(f"RAG ê²°ê³¼ ë¬¸ì„œ ìˆ˜: {len(docs)}")

    enough, diag = assess_sufficiency(docs, req)
    st.write("**ì¶©ë¶„ì„± íŒë‹¨** â†’ ", "ì¶©ë¶„ âœ…" if enough else "ë¶€ì¡± âŒ", diag)

    # Aì•ˆ: í†µí•© Evidence
    combined_ev: List[EvidenceItem] = []
    # RAGì—ì„œ ìš°ì„  ìˆ˜ì§‘
    combined_ev.extend(_evidence_from_docs(docs, search_query_text, keep_len=1200))

    if not enough:
        st.info("ì¶©ë¶„ì„± ë¶€ì¡± â†’ ì›¹ ë”¥ë¦¬ì„œì¹˜(Evidence ë³´ê°•) ìˆ˜í–‰")
        policy = st.session_state.get("dr_policy", {})
        dr_ev = gather_dr_evidence(search_query_text, policy, keep_len=1200)
        combined_ev.extend(dr_ev)

    # ì¤‘ë³µ ì œê±° ë° ë„ë©”ì¸ ì œí•œ
    combined_ev = dedup_evidence(combined_ev)
    combined_ev = _limit_refs_by_domain(combined_ev, max_total=12, max_per_domain=3)

    # ë‹¨ì¼ ë³´ê³ ì„œ ìƒì„± (TTD-DR ìµœì¢…)
    with st.spinner("ğŸ§¬ TTD-DR ë³´ê³ ì„œ ìƒì„± ì¤‘â€¦(í†µí•© Evidence)"):
        merged_md = _compose_enhanced_markdown_from_evidence(st.session_state.llm, topic_title, combined_ev)

    st.session_state.final_report_md = merged_md
    st.session_state.report_topic = topic_title
    st.success("âœ… ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")

# ë³´ê³ ì„œ ì¶œë ¥
if st.session_state.get("final_report_md"):
    st.markdown("---")
    st.markdown("## ğŸ“‹ **ìµœì¢… ë³´ê³ ì„œ (TTD-DR, í†µí•© Evidence)**")
    st.download_button(
        "ğŸ“¥ ë‹¤ìš´ë¡œë“œ (Markdown)",
        data=st.session_state.final_report_md.encode("utf-8"),
        file_name=f"final_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.md",
        mime="text/markdown",
    )
    with st.expander("ğŸ“– ë³´ê³ ì„œ ë¯¸ë¦¬ë³´ê¸°", expanded=True):
        st.markdown(st.session_state.final_report_md)
